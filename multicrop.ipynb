{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import multicrop\n",
    "import constants\n",
    "import data\n",
    "import tokenizer\n",
    "from datasets import emoreact, enterface\n",
    "reload(multicrop)\n",
    "reload(data)\n",
    "reload(constants)\n",
    "reload(tokenizer)\n",
    "reload(emoreact)\n",
    "reload(enterface)\n",
    "from data import load_data \n",
    "from tokenizer import save_video_frames_tensors\n",
    "from constants import *\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SZ = 3\n",
    "SPLIT = [1, 0.0, 0.0]\n",
    "dataset_to_use = enterface\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/depmbt/data.py:46: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  return self.dataset.iloc[item][0], self.dataset.iloc[item][1], self.dataset.iloc[item][2]\n",
      "/root/miniconda/envs/depmbt/lib/python3.9/site-packages/librosa/util/decorators.py:88: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  return f(*args, **kwargs)\n",
      "/root/miniconda/envs/depmbt/lib/python3.9/site-packages/librosa/util/decorators.py:88: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  return f(*args, **kwargs)\n",
      "/root/miniconda/envs/depmbt/lib/python3.9/site-packages/librosa/util/decorators.py:88: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dl, val_dl, test_dl  = load_data(dataset_to_use, \n",
    "                                       batch_sz=BATCH_SZ,\n",
    "                                       train_val_test_split=SPLIT)\n",
    "def get_image_seq(crops, i, rgb=True):\n",
    "\n",
    "    if rgb:\n",
    "        image_seq = [ \n",
    "            crops[i][j].reshape(FRAMES, CHANS, HEIGHT, WIDTH)[k]\n",
    "                for j in range(BATCH_SZ) for k in range(3)\n",
    "        ]\n",
    "\n",
    "    else:\n",
    "        image_seq = [\n",
    "            crops[i][j].reshape(CHANS, NUM_MELS, dataset_to_use.MAX_SPEC_SEQ_LEN)\n",
    "                for j in range(BATCH_SZ)\n",
    "        ]\n",
    "\n",
    "    return image_seq\n",
    "\n",
    "def get_stacked_rgb(crops):\n",
    "    global_crop_seq = []\n",
    "    for i in range(len(crops)):\n",
    "        image_seq_rgb = get_image_seq(crops, i, rgb=True)\n",
    "        # stacked_frames = torch.cat(image_seq_rgb, dim=0)\n",
    "        # global_crop_seq.append(stacked_frames)\n",
    "        global_crop_seq  += image_seq_rgb\n",
    "    global_stacked = torch.cat(global_crop_seq, dim=0)\n",
    "    global_stacked = global_stacked.reshape(-1, BATCH_SZ, HEIGHT, WIDTH)\n",
    "    return global_stacked\n",
    "    \n",
    "def get_stacked_spec(crops):\n",
    "    global_crop_seq = []\n",
    "    for i in range(len(crops)):\n",
    "        image_seq_rgb = get_image_seq(crops, i, rgb=False)\n",
    "        # stacked_frames = torch.cat(image_seq_rgb, dim=0)\n",
    "        # global_crop_seq.append(stacked_frames)\n",
    "        global_crop_seq  += image_seq_rgb\n",
    "    global_stacked = torch.cat(global_crop_seq, dim=0)\n",
    "    global_stacked = global_stacked.reshape(-1, BATCH_SZ, NUM_MELS, dataset_to_use.MAX_SPEC_SEQ_LEN)\n",
    "    return global_stacked\n",
    "\n",
    "for data in train_dl:\n",
    "    crops = data[\"student_spec\"]\n",
    "    crops = data[\"teacher_spec\"]\n",
    "    crops = data[\"teacher_rgb\"]\n",
    "    crops = data[\"student_rgb\"]\n",
    "    global_stacked = get_stacked_rgb(crops)\n",
    "    break\n",
    "\n",
    "    # for video, audio in zip(data[\"teacher_rgb\"], data[\"teacher_spec\"]):\n",
    "    #     video_frames = video[0].reshape(FRAMES, CHANS, HEIGHT, WIDTH)\n",
    "    #     audio_frame = audio[0]\n",
    "    #     break\n",
    "    # break\n",
    "shape = global_stacked.shape\n",
    "save_video_frames_tensors(global_stacked, \"frames_v\", rows=int(shape[0] / NUM_LOCAL_VIEWS))\n",
    "\n",
    "# save_video_frames_tensors(audio_frame, \"frame_a\")\n",
    "# save_video_frames_tensors(video_frames, \"frames_v\")\n",
    "    # total_items = len(global_crops) + len(local_crops)\n",
    "# stacked_local = torch.cat(local_crops, dim=2)\n",
    "# plt.imshow(global_stacked.swapaxes(0, 2).swapaxes(0, 1))\n",
    "# plt.imshow(global_stacked)\n",
    "# plt.imshow(stacked_local.swapaxes(0, 2).swapaxes(0, 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "depmbt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
