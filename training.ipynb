{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import BCELoss\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "import vitmbt\n",
    "import mbt\n",
    "import ablation\n",
    "import helpers\n",
    "reload(mbt)\n",
    "reload(ablation)\n",
    "reload(helpers)\n",
    "reload(vitmbt)\n",
    "from helpers import ClassifierMetrics, display_tensor_as_rgb\n",
    "from vitmbt import ViTMBT, train, val\n",
    "from data import EmoDataset, new_collate_fn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading audio layers...\n",
      "Loaded successfully in 3.8s\n",
      "Loading video layers...\n",
      "Loaded successfully in 3.8s\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 23.69 GiB total capacity; 11.72 GiB already allocated; 9.81 MiB free; 11.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m LABELS \u001b[39m=\u001b[39m \u001b[39m8\u001b[39m\n\u001b[1;32m      7\u001b[0m vmbt \u001b[39m=\u001b[39m ViTMBT(\u001b[39m1024\u001b[39m, num_class\u001b[39m=\u001b[39mLABELS)\n\u001b[0;32m----> 8\u001b[0m vmbt \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mDataParallel(vmbt)\u001b[39m.\u001b[39;49mcuda()\n\u001b[1;32m     10\u001b[0m se \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     11\u001b[0m lines \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/depmbt/lib/python3.9/site-packages/torch/nn/modules/module.py:680\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcuda\u001b[39m(\u001b[39mself\u001b[39m: T, device: Optional[Union[\u001b[39mint\u001b[39m, device]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m    664\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    665\u001b[0m \n\u001b[1;32m    666\u001b[0m \u001b[39m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[1;32m    679\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 680\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(\u001b[39mlambda\u001b[39;49;00m t: t\u001b[39m.\u001b[39;49mcuda(device))\n",
      "File \u001b[0;32m~/miniconda/envs/depmbt/lib/python3.9/site-packages/torch/nn/modules/module.py:570\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    569\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 570\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    572\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    573\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    574\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    575\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    581\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/depmbt/lib/python3.9/site-packages/torch/nn/modules/module.py:570\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    569\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 570\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    572\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    573\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    574\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    575\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    581\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 570 (4 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda/envs/depmbt/lib/python3.9/site-packages/torch/nn/modules/module.py:570\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    569\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 570\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    572\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    573\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    574\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    575\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    581\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/depmbt/lib/python3.9/site-packages/torch/nn/modules/module.py:593\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    590\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    591\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    592\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 593\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    594\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    595\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/miniconda/envs/depmbt/lib/python3.9/site-packages/torch/nn/modules/module.py:680\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcuda\u001b[39m(\u001b[39mself\u001b[39m: T, device: Optional[Union[\u001b[39mint\u001b[39m, device]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m    664\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    665\u001b[0m \n\u001b[1;32m    666\u001b[0m \u001b[39m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[1;32m    679\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 680\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply(\u001b[39mlambda\u001b[39;00m t: t\u001b[39m.\u001b[39;49mcuda(device))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 23.69 GiB total capacity; 11.72 GiB already allocated; 9.81 MiB free; 11.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\"\n",
    "PRETRAINED_CHKPT = \"./pretrained_models/L_16-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.1-sd_0.1--imagenet2012-steps_20k-lr_0.01-res_224.npz\"\n",
    "EPOCHS = 200\n",
    "lr = 0.00001\n",
    "BATCH_SZ = 2\n",
    "LABELS = 8\n",
    "vmbt = ViTMBT(1024, num_class=LABELS)\n",
    "vmbt = nn.DataParallel(vmbt).cuda()\n",
    "\n",
    "se = None\n",
    "lines = None\n",
    "train_ds = EmoDataset(f\"{DATA_DIR}/Labels/train_labels_full_pruned.csv\", nlines=lines, sole_emotion=se)\n",
    "val_ds = EmoDataset(f\"{DATA_DIR}/Labels/val_labels_full_pruned.csv\", nlines=lines, sole_emotion=se)\n",
    "test_ds = EmoDataset(f\"{DATA_DIR}/Labels/test_labels_full_pruned.csv\", nlines=lines, sole_emotion=se)\n",
    "\n",
    "train_metrics = ClassifierMetrics(task='multilabel', n_labels=LABELS, device=DEVICE)\n",
    "val_metrics = ClassifierMetrics(task='multilabel', n_labels=LABELS, device=DEVICE)\n",
    "\n",
    "train_dl = DataLoader(train_ds, collate_fn=new_collate_fn, batch_size=BATCH_SZ, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, collate_fn=new_collate_fn, batch_size=BATCH_SZ, shuffle=False)\n",
    "test_dl = DataLoader(test_ds, collate_fn=new_collate_fn, batch_size=BATCH_SZ, shuffle=False)\n",
    "\n",
    "optimizer = torch.optim.AdamW(vmbt.parameters(), betas=(0.9, 0.999), lr=lr, weight_decay=1.0 / BATCH_SZ)\n",
    "loss_func = BCELoss()\n",
    "# loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8813e60b64f546b8b70880334eb15738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 1, 8])\n",
      "torch.Size([1, 2048])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 1, 8])\n",
      "torch.Size([1, 2048])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 1, 8])\n",
      "torch.Size([1, 2048])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 1, 8])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 23.69 GiB total capacity; 11.70 GiB already allocated; 9.81 MiB free; 11.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCHS):\n\u001b[0;32m----> 2\u001b[0m     train_metrics \u001b[39m=\u001b[39m train(vmbt, train_dl, optimizer, loss_fn\u001b[39m=\u001b[39;49mloss_func, cls_metrics\u001b[39m=\u001b[39;49mtrain_metrics)\n\u001b[1;32m      3\u001b[0m     train_loss, train_f1, train_r, train_p, train_acc, train_bottleneck_tokens \u001b[39m=\u001b[39m train_metrics\n\u001b[1;32m      4\u001b[0m     val_metrics \u001b[39m=\u001b[39m val(vmbt, val_dl, loss_fn\u001b[39m=\u001b[39mloss_func, cls_metrics\u001b[39m=\u001b[39mval_metrics)\n",
      "File \u001b[0;32m~/depmbt/vitmbt.py:237\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(net, trainldr, optimizer, loss_fn, cls_metrics)\u001b[0m\n\u001b[1;32m    235\u001b[0m y, bot_token \u001b[39m=\u001b[39m net(audio, video)\n\u001b[1;32m    236\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(y, labels)\n\u001b[0;32m--> 237\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    238\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    239\u001b[0m total_losses\u001b[39m.\u001b[39mupdate(loss\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mitem(), audio\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda/envs/depmbt/lib/python3.9/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    300\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    301\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    306\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 307\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/miniconda/envs/depmbt/lib/python3.9/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m--> 154\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[1;32m    155\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    156\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/miniconda/envs/depmbt/lib/python3.9/site-packages/torch/autograd/function.py:199\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mImplementing both \u001b[39m\u001b[39m'\u001b[39m\u001b[39mbackward\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvjp\u001b[39m\u001b[39m'\u001b[39m\u001b[39m for a custom \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    196\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mFunction is not allowed. You should only implement one \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    197\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mof them.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    198\u001b[0m user_fn \u001b[39m=\u001b[39m vjp_fn \u001b[39mif\u001b[39;00m vjp_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m Function\u001b[39m.\u001b[39mvjp \u001b[39melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 199\u001b[0m \u001b[39mreturn\u001b[39;00m user_fn(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m~/miniconda/envs/depmbt/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:34\u001b[0m, in \u001b[0;36mBroadcast.backward\u001b[0;34m(ctx, *grad_outputs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackward\u001b[39m(ctx, \u001b[39m*\u001b[39mgrad_outputs):\n\u001b[0;32m---> 34\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mNone\u001b[39;00m,) \u001b[39m+\u001b[39m ReduceAddCoalesced\u001b[39m.\u001b[39;49mapply(ctx\u001b[39m.\u001b[39;49minput_device, ctx\u001b[39m.\u001b[39;49mnum_inputs, \u001b[39m*\u001b[39;49mgrad_outputs)\n",
      "File \u001b[0;32m~/miniconda/envs/depmbt/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:45\u001b[0m, in \u001b[0;36mReduceAddCoalesced.forward\u001b[0;34m(ctx, destination, num_inputs, *grads)\u001b[0m\n\u001b[1;32m     41\u001b[0m ctx\u001b[39m.\u001b[39mtarget_gpus \u001b[39m=\u001b[39m [grads[i]\u001b[39m.\u001b[39mget_device() \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(grads), num_inputs)]\n\u001b[1;32m     43\u001b[0m grads_ \u001b[39m=\u001b[39m [grads[i:i \u001b[39m+\u001b[39m num_inputs]\n\u001b[1;32m     44\u001b[0m           \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(grads), num_inputs)]\n\u001b[0;32m---> 45\u001b[0m \u001b[39mreturn\u001b[39;00m comm\u001b[39m.\u001b[39;49mreduce_add_coalesced(grads_, destination)\n",
      "File \u001b[0;32m~/miniconda/envs/depmbt/lib/python3.9/site-packages/torch/nn/parallel/comm.py:143\u001b[0m, in \u001b[0;36mreduce_add_coalesced\u001b[0;34m(inputs, destination, buffer_size)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[39mfor\u001b[39;00m chunks \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mitrs):\n\u001b[1;32m    142\u001b[0m     flat_tensors \u001b[39m=\u001b[39m [_flatten_dense_tensors(chunk) \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m chunks]  \u001b[39m# (num_gpus,)\u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m     flat_result \u001b[39m=\u001b[39m reduce_add(flat_tensors, destination)\n\u001b[1;32m    144\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m _unflatten_dense_tensors(flat_result, chunks[\u001b[39m0\u001b[39m]):\n\u001b[1;32m    145\u001b[0m         \u001b[39m# The unflattened tensors do not share storage, and we don't expose\u001b[39;00m\n\u001b[1;32m    146\u001b[0m         \u001b[39m# base flat tensor anyways, so give them different version counters.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m         \u001b[39m# See NOTE [ Version Counter in comm.*_coalesced ]\u001b[39;00m\n\u001b[1;32m    148\u001b[0m         output\u001b[39m.\u001b[39mappend(t\u001b[39m.\u001b[39mdata)\n",
      "File \u001b[0;32m~/miniconda/envs/depmbt/lib/python3.9/site-packages/torch/nn/parallel/comm.py:95\u001b[0m, in \u001b[0;36mreduce_add\u001b[0;34m(inputs, destination)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[39mreturn\u001b[39;00m inputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m     94\u001b[0m \u001b[39mif\u001b[39;00m nccl\u001b[39m.\u001b[39mis_available(inputs):\n\u001b[0;32m---> 95\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mempty_like(inputs[root_index])\n\u001b[1;32m     96\u001b[0m     nccl\u001b[39m.\u001b[39mreduce(inputs, output\u001b[39m=\u001b[39mresult, root\u001b[39m=\u001b[39mroot_index)\n\u001b[1;32m     97\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 23.69 GiB total capacity; 11.70 GiB already allocated; 9.81 MiB free; 11.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    train_metrics = train(vmbt, train_dl, optimizer, loss_fn=loss_func, cls_metrics=train_metrics)\n",
    "    train_loss, train_f1, train_r, train_p, train_acc, train_bottleneck_tokens = train_metrics\n",
    "    val_metrics = val(vmbt, val_dl, loss_fn=loss_func, cls_metrics=val_metrics)\n",
    "    val_loss, val_f1, val_r, val_p, val_acc, val_bottleneck_tokens = val_metrics\n",
    "    train_token = train_bottleneck_tokens[0][:, :256]\n",
    "    val_token = val_bottleneck_tokens[0][:, :256]\n",
    "\n",
    "    print(\n",
    "        (f\"Epoch {epoch + 1}: train_loss {train_loss:.5f}, val_loss {val_loss:.5f}\\n\"\n",
    "            f\"                   train_precision {train_p}, val_precision {val_p}\\n\"\n",
    "            f\"                   train_recall {train_r}, val_recall {val_r}\\n\"\n",
    "            f\"                   train_f1 {train_f1}, val_f1 {val_f1}\"\n",
    "            )\n",
    "        )\n",
    "    display_tensor_as_rgb(train_token, \"Train bottleneck token\") \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "depmbt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
